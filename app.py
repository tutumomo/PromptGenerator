import streamlit as st
from openai import OpenAI
import os
import sys
import subprocess
import google.generativeai as genai
import requests
from typing import List
from google.api_core import retry
import json
from dotenv import load_dotenv

load_dotenv()  # è¼‰å…¥ .env æ–‡ä»¶ä¸­çš„ç’°å¢ƒè®Šæ•¸

# ä¿®æ”¹ system prompt å¸¸é‡
PROMPT_IMPROVEMENT_TEMPLATE = '''
# è§’è‰² 
ä½ æ˜¯ä¸€ä½AI prompt å°ˆå®¶ï¼Œç†Ÿæ‚‰å„ç¨®Prompt Optimizeræ¡†æ¶(APEã€CAREã€CHATã€COASTã€CREATã€CRISPEã€RASCEFã€RTFç‚ºä¸»)ï¼Œå¯ä»¥å°‡ä½¿ç”¨è€…è¼¸å…¥çš„æç¤ºè©é¸å®šåˆé©çš„Promptæ¡†æ¶å¾Œï¼Œç·¨æ’°å’Œå„ªåŒ–AI promptsã€‚
## é‡è¦:
- ç„¡è«–æå•ä½¿ç”¨ä½•ç¨®èªè¨€ï¼Œä¸€å¾‹ä»¥ç¹é«”ä¸­æ–‡é€²è¡Œå›ç­”(ç¦ç”¨ç°¡é«”ä¸­æ–‡ï¼Œä¸”é ˆç¬¦åˆå°ç£ç”¨èªç¿’æ…£)ã€‚ 

è«‹åˆ†æä¸¦æ”¹é€²ä»¥ä¸‹æç¤ºè©:
{original_prompt}

è«‹ç›´æ¥è¿”å›å„ªåŒ–å¾Œçš„æç¤ºè©ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–èªªæ˜æ–‡å­—ã€æ¨™é¡Œæˆ–è§£é‡‹(è¿”å›çµæœä¸è¦æœ‰"# å„ªåŒ–å¾Œæç¤ºè©ï¼š"çš„å­—çœ¼).
'''

# OpenAI API èª¿ç”¨å‡½æ•¸
def call_openai(api_key, model, prompt, temperature, top_p, max_tokens, presence_penalty, frequency_penalty):
    """
    èª¿ç”¨ OpenAI API ä¾†æ”¹é€²æç¤ºè©
    """
    client = OpenAI(api_key=api_key)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æç¤ºè©å·¥ç¨‹å¸«ï¼Œå°ˆé–€å¹«åŠ©æ”¹é€²å’Œå„ªåŒ–æç¤ºè©ã€‚"},
            {"role": "user", "content": PROMPT_IMPROVEMENT_TEMPLATE.format(original_prompt=prompt)}
        ],
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        presence_penalty=presence_penalty,
        frequency_penalty=frequency_penalty
    )
    return response.choices[0].message.content

# Gemini API èª¿ç”¨å‡½æ•¸
def call_gemini(api_key: str, model_name: str, prompt: str, temperature: float, top_p: float, max_tokens: int) -> str:
    """
    èª¿ç”¨ Gemini API ä¾†æ”¹é€²æç¤ºè©
    """
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)
        
        prompt_template = PROMPT_IMPROVEMENT_TEMPLATE.format(original_prompt=prompt)
        
        response = model.generate_content(
            prompt_template,
            generation_config=genai.types.GenerationConfig(
                temperature=temperature,
                top_p=top_p,
                max_output_tokens=max_tokens,
                candidate_count=1
            )
        )
        
        return response.text
    except Exception as e:
        return f"Gemini API éŒ¯èª¤: {str(e)}"

# xAI API èª¿ç”¨å‡½æ•¸
def call_xai(api_key, prompt, temperature, top_p, max_tokens):
    """
    èª¿ç”¨ xAI API
    æ³¨æ„ï¼šç›®å‰ xAI/Grok API å°šæœªå…¬é–‹
    """
    # è¿”å›æç¤ºä¿¡æ¯
    return "xAI (Grok) API ç›®å‰å°šæœªå…¬é–‹ï¼Œä½†å·²çŸ¥æœ‰ grok-beta åŠ grok-vision-beta å…©å€‹æ¨¡å‹ã€‚è«‹ç­‰å¾…å®˜æ–¹ç™¼å¸ƒ API å­˜å–æ–¹å¼ã€‚"

# Ollama API èª¿ç”¨å‡½æ•¸
def call_ollama(model_name, prompt, temperature):
    """
    èª¿ç”¨æœ¬åœ° Ollama API ä¾†æ”¹é€²æç¤ºè©
    """
    try:
        prompt_template = PROMPT_IMPROVEMENT_TEMPLATE.format(original_prompt=prompt)
        
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                "model": model_name,
                "prompt": prompt_template,
                "temperature": temperature,
                "stream": False
            },
            stream=False
        )
        
        if response.status_code == 200:
            try:
                return response.json().get('response', 'ç”Ÿæˆå¤±æ•—ï¼šç„¡å›æ‡‰å…§å®¹')
            except json.JSONDecodeError as e:
                full_response = []
                for line in response.text.strip().split('\n'):
                    if line:
                        try:
                            data = json.loads(line)
                            if 'response' in data:
                                full_response.append(data['response'])
                        except json.JSONDecodeError:
                            continue
                return ''.join(full_response) if full_response else 'ç”Ÿæˆå¤±æ•—ï¼šå›æ‡‰æ ¼å¼éŒ¯èª¤'
        else:
            return f'ç”Ÿæˆå¤±æ•—ï¼šHTTP éŒ¯èª¤ {response.status_code}'
            
    except Exception as e:
        return f"èª¿ç”¨ Ollama å¤±æ•—: {str(e)}"

# æ–°å¢å‡½æ•¸ä¾†ç²å– OpenAI æ¨¡å‹åˆ—è¡¨
def get_openai_models(api_key: str) -> List[str]:
    """
    ç²å– OpenAI å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    """
    try:
        client = OpenAI(api_key=api_key)
        models = client.models.list()
        # ç¯©é¸å‡ºæ‰€æœ‰å¯ç”¨æ–¼èŠå¤©çš„æ¨¡å‹
        chat_models = []
        for model in models:
            if ('gpt' in model.id.lower() and 
                not any(x in model.id.lower() for x in ['instruct', 'similarity', 'edit', 'audio'])):
                chat_models.append(model.id)
        return sorted(chat_models) if chat_models else ["gpt-3.5-turbo", "gpt-4"]
    except Exception as e:
        st.error(f"ç²å– OpenAI æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}")
        return ["gpt-3.5-turbo", "gpt-4"]

# æ–°å¢å‡½æ•¸ä¾†ç²å– Ollama æ¨¡å‹åˆ—è¡¨
def get_ollama_models() -> List[str]:
    """
    ç²å–æœ¬åœ° Ollama å·²å®‰è£çš„æ¨¡å‹åˆ—è¡¨
    """
    try:
        response = requests.get('http://localhost:11434/api/tags')
        if response.status_code == 200:
            models = [model['name'] for model in response.json()['models']]
            return sorted(models) if models else ["aya-expanse"]
        return ["aya-expanse"]
    except Exception as e:
        st.error(f"ç²å– Ollama æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}")
        return ["aya-expanse"]  # è¿”å›é»˜èªæ¨¡å‹

# æ–°å¢ Gemini æ¨¡å‹åˆ—è¡¨ç²å–å‡½æ•¸
def get_gemini_models(api_key: str) -> List[str]:
    """
    ç²å– Gemini å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    """
    try:
        genai.configure(api_key=api_key)
        # ç›®å‰å·²çŸ¥çš„ Gemini æ¨¡å‹
        default_models = ["gemini-pro", "gemini-pro-vision"]
        
        # å˜—è©¦ç²å–æ¨¡å‹åˆ—è¡¨
        try:
            models = genai.list_models()
            gemini_models = []
            for model in models:
                if 'gemini' in model.name.lower():
                    gemini_models.append(model.name)
            return sorted(gemini_models) if gemini_models else default_models
        except:
            return default_models
            
    except Exception as e:
        st.error(f"ç²å– Gemini æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}")
        return ["gemini-pro"]  # è¿”å›æœ€åŸºæœ¬çš„æ¨¡å‹

# æ–°å¢ xAI æ¨¡å‹åˆ—è¡¨ç²å–å‡½æ•¸
def get_xai_models(api_key: str) -> List[str]:
    """
    ç²å– xAI å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    æ³¨æ„ï¼šç›®å‰ xAI/Grok API å°šæœªå…¬é–‹ï¼Œè¿”å›å·²çŸ¥æ¨¡å‹åˆ—è¡¨
    """
    # ç›®å‰å·²çŸ¥çš„ Grok æ¨¡å‹
    default_models = ["grok-beta", "grok-vision-beta"]
    
    try:
        # æš«æ™‚ä¸é€²è¡Œ API èª¿ç”¨ï¼Œç›´æ¥è¿”å›å·²çŸ¥æ¨¡å‹
        return default_models
    except Exception as e:
        st.error(f"ç²å– xAI æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}")
        return default_models

# æ–°å¢å‡½æ•¸ä¾†æª¢æŸ¥ API é‡‘é‘°
def get_api_key(api_type: str) -> tuple[str, str]:
    """
    æª¢æŸ¥ä¸¦ç²å– API é‡‘é‘°
    è¿”å›: (api_key, message)
    """
    env_var_map = {
        "OpenAI": "OPENAI_API_KEY",
        "Gemini": "GEMINI_API_KEY",
        "xAI": "XAI_API_KEY"
    }
    
    env_var = env_var_map.get(api_type)
    if not env_var:
        return None, ""
        
    api_key = os.getenv(env_var)
    if api_key:
        return api_key, f"âœ… å·²å¾ç’°å¢ƒè®Šæ•¸ {env_var} è®€å– API é‡‘é‘°"
    return None, f"ğŸ’¡ å¯ä»¥è¨­ç½®ç’°å¢ƒè®Šæ•¸ {env_var} ä¾†å„²å­˜ API é‡‘é‘°"

# æ–°å¢åŸ·è¡Œæç¤ºè©çš„å‡½æ•¸
def execute_prompt(api_type: str, api_key: str, model: str, prompt: str, 
                  temperature: float, top_p: float, max_tokens: int,
                  presence_penalty: float = 0, frequency_penalty: float = 0) -> str:
    """
    åŸ·è¡Œæç¤ºè©ä¸¦å›ç”Ÿæˆçš„å…§å®¹
    """
    try:
        if api_type == "OpenAI":
            client = OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty
            )
            return response.choices[0].message.content
        
        elif api_type == "Gemini":
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(model)
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=temperature,
                    top_p=top_p,
                    max_output_tokens=max_tokens
                )
            )
            return response.text
        
        elif api_type == "Ollama":
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    "model": model,
                    "prompt": prompt,
                    "temperature": temperature,
                    "stream": False
                }
            )
            return response.json().get('response', 'ç”Ÿæˆå¤±æ•—ï¼šç„¡å›æ‡‰å…§å®¹')
        
        else:  # xAI
            return "xAI (Grok) API ç›®å‰å°šæœªå…¬é–‹"
            
    except Exception as e:
        return f"ç”Ÿæˆå¤±æ•—: {str(e)}"

def main():
    st.title("AI æç¤ºè©å„ªåŒ–å™¨")
    
    with st.sidebar:
        st.header("è¨­å®š")
        api_options = ["Ollama", "OpenAI", "Gemini", "xAI"]  # ä¿®æ”¹é †åºï¼Œå°‡ Ollama æ”¾åœ¨ç¬¬ä¸€ä½
        selected_api = st.selectbox("é¸æ“‡ API:", api_options, index=0)  # index=0 æœƒé¸æ“‡ Ollama ä½œç‚ºé è¨­

        # API é‡‘é‘°è™•ç†
        if selected_api != "Ollama":
            api_key, env_message = get_api_key(selected_api)
            if env_message:
                st.info(env_message)
            
            # å¦‚æœç’°å¢ƒè®Šæ•¸ä¸­æ²’æœ‰ API é‡‘é‘°ï¼Œå‰‡é¡¯ç¤ºè¼¸å…¥æ¡†
            if not api_key:
                api_key = st.text_input(f"è¼¸å…¥ {selected_api} API é‡‘é‘°:", type="password")
        else:
            api_key = None

        # æ¨¡å‹é¸æ“‡
        if selected_api == "OpenAI":
            if api_key:
                model_options = get_openai_models(api_key)
            else:
                model_options = ["gpt-3.5-turbo", "gpt-4"]
            selected_model = st.selectbox("é¸æ“‡æ¨¡å‹:", model_options)
        elif selected_api == "Gemini":
            if api_key:
                model_options = get_gemini_models(api_key)
            else:
                model_options = ["gemini-pro"]
            selected_model = st.selectbox("é¸æ“‡æ¨¡å‹:", model_options)
        elif selected_api == "xAI":
            if api_key:
                model_options = get_xai_models(api_key)
            else:
                model_options = ["grok-beta", "grok-vision-beta"]
            selected_model = st.selectbox("é¸æ“‡æ¨¡å‹:", model_options)
            st.warning("âš ï¸ xAI (Grok) API ç›®å‰å°šæœªå…¬é–‹ï¼Œä½†å·²çŸ¥æœ‰ grok-beta åŠ grok-vision-beta å…©å€‹æ¨¡å‹ã€‚è«‹ç­‰å¾…å®˜æ–¹ç™¼å¸ƒ API å­˜å–æ–¹å¼ã€‚")
        else:  # Ollama
            model_options = get_ollama_models()
            selected_model = st.selectbox("é¸æ“‡æœ¬åœ°æ¨¡å‹:", model_options, 
                                        index=model_options.index("aya-expanse") if "aya-expanse" in model_options else 0)

        # åƒæ•¸è¨­å®š
        st.header("åƒæ•¸è¨­å®š")
        temperature = st.slider("Temperature:", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
        top_p = st.slider("Top P:", min_value=0.0, max_value=1.0, value=0.9, step=0.1)
        max_tokens = st.slider("Max Tokens:", min_value=50, max_value=6000, value=2000, step=50)
        
        if selected_api == "OpenAI":
            presence_penalty = st.slider("Presence Penalty:", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)
            frequency_penalty = st.slider("Frequency Penalty:", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)
        else:
            presence_penalty = 0
            frequency_penalty = 0

    # ä¸»è¦å…§å®¹å€åŸŸ
    st.header("è¼¸å…¥åŸå§‹æç¤ºè©")
    original_prompt = st.text_area(
        "è«‹è¼¸å…¥ä½ æƒ³è¦å„ªåŒ–çš„æç¤ºè©:",
        help="è¼¸å…¥ä½ çš„åŸå§‹æç¤ºè©ï¼ŒAI å°‡å¹«åŠ©ä½ æ”¹é€²ä½¿å…¶æ›´åŠ æœ‰æ•ˆã€‚",
        height=150
    )

    # å­˜å„²å„ªåŒ–å¾Œçš„æç¤ºè©çš„ session state
    if 'improved_prompt' not in st.session_state:
        st.session_state.improved_prompt = ""

    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("å„ªåŒ–æç¤ºè©"):
            if not original_prompt:
                st.error("è«‹è¼¸å…¥åŸå§‹æç¤ºã€‚")
                return

            if not api_key and selected_api != "Ollama":
                st.error(f"è«‹è¼¸å…¥æœ‰æ•ˆçš„ {selected_api} API Keyã€‚")
                return

            try:
                with st.spinner('å„ªåŒ–ä¸­...'):
                    if selected_api == "OpenAI":
                        response = call_openai(api_key, selected_model, original_prompt, 
                                             temperature, top_p, max_tokens, 
                                             presence_penalty, frequency_penalty)
                    elif selected_api == "Gemini":
                        response = call_gemini(api_key, selected_model, original_prompt, 
                                             temperature, top_p, max_tokens)
                    elif selected_api == "xAI":
                        response = call_xai(api_key, original_prompt, temperature, 
                                          top_p, max_tokens)
                    else:  # Ollama
                        response = call_ollama(selected_model, original_prompt, temperature)

                    st.session_state.improved_prompt = response
            except Exception as e:
                st.error(f"å„ªåŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                return

    # ä¿®æ”¹é¡¯ç¤ºå„ªåŒ–å¾Œçš„æç¤ºè©å’Œç”ŸæˆæŒ‰éˆ•çš„éƒ¨åˆ†
    if st.session_state.improved_prompt:
        st.subheader("å„ªåŒ–å¾Œçš„æç¤ºè©")
        improved_prompt = st.text_area(
            "å„ªåŒ–çµæœ:",
            value=st.session_state.improved_prompt,
            height=150
        )
        
        # æ·»åŠ å¯é¸çš„ç”¨æˆ¶è¼¸å…¥æ¡†
        user_input = st.text_input(
            "è«‹è¼¸å…¥ä½ çš„éœ€æ±‚: (å¯é¸)",
            help="å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨æ­¤è¼¸å…¥å…·é«”éœ€æ±‚ã€‚ä¾‹å¦‚ï¼šæƒ³è¦ä¸€é“é¦™è¾£å¤ å‘³çš„å®®ä¿é›ä¸é£Ÿè­œ"
        )
        
        # ç”Ÿæˆå…§å®¹æŒ‰éˆ•
        if st.button("åŸ·è¡Œæç¤ºè©"):
            with st.spinner('ç”Ÿæˆä¸­...'):
                # æ ¹æ“šæ˜¯å¦æœ‰ç”¨æˆ¶è¼¸å…¥ä¾†çµ„åˆæœ€çµ‚çš„æç¤ºè©
                final_prompt = (
                    f"{improved_prompt}\n\nç”¨æˆ¶è¼¸å…¥ï¼š{user_input}"
                    if user_input
                    else improved_prompt
                )
                
                generated_content = execute_prompt(
                    selected_api, api_key, selected_model, final_prompt,
                    temperature, top_p, max_tokens, presence_penalty, frequency_penalty
                )
                st.subheader("ç”Ÿæˆçš„å…§å®¹")
                # ä½¿ç”¨ markdown é¡¯ç¤ºç”Ÿæˆçš„å…§å®¹
                st.markdown(generated_content)
                # ä¿ç•™åŸå§‹æ–‡æœ¬é¡¯ç¤ºï¼Œæ–¹ä¾¿è¤‡è£½
                with st.expander("é¡¯ç¤ºåŸå§‹æ–‡æœ¬ï¼ˆæ–¹ä¾¿è¤‡è£½ï¼‰"):
                    st.text_area("åŸå§‹æ–‡æœ¬:", value=generated_content, height=300)

if __name__ == "__main__":
    main()
