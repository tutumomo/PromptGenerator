# ä¿®æ”¹ import å€å¡Š
import streamlit as st
import requests
from typing import List, Tuple
from dotenv import load_dotenv
import json
from datetime import datetime
import pathlib
import os
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

load_dotenv()  # è¼‰å…¥ .env æ–‡ä»¶ä¸­çš„ç’°å¢ƒè®Šæ•¸

# ç’°å¢ƒè®Šæ•¸æ˜ å°„
ENV_VAR_MAP = {
    "X.AI": "XAI_API_KEY",
    "Mistral": "MISTRAL_API_KEY"
}

XAI_MODELS = [
    "grok-2",           # æœ€æ–°çš„ grok-2 æ¨¡å‹
    "grok-2-vision-1212", # æ”¯æ´è¦–è¦ºåŠŸèƒ½çš„ grok-2
    "grok-beta",        # åŸå§‹çš„ grok æ¨¡å‹
    "grok-vision-beta"  # åŸå§‹çš„è¦–è¦ºæ¨¡å‹
]

def get_api_key(api_type: str) -> Tuple[str, str]:
    """
    æª¢æŸ¥ä¸¦ç²å– API é‡‘é‘°
    è¿”å›: (api_key, message)
    """
    env_var = ENV_VAR_MAP.get(api_type)
    if not env_var:
        return None, ""
        
    api_key = os.getenv(env_var)
    if api_key:
        return api_key, f"âœ… å·²å¾ç’°å¢ƒè®Šæ•¸ {env_var} è®€å– API é‡‘é‘°"
    return None, f"ğŸ’¡ å¯ä»¥è¨­ç½®ç’°å¢ƒè®Šæ•¸ {env_var} ä¾†å„²å­˜ API é‡‘é‘°"

# ä¿®æ”¹ system prompt å¸¸é‡
PROMPT_IMPROVEMENT_TEMPLATE = '''
# è§’è‰² 
ä½ æ˜¯ä¸€ä½AI prompt å°ˆå®¶ï¼Œç†Ÿæ‚‰å„ç¨®Prompt Optimizeræ¡†æ¶(APEã€CAREã€CHATã€COASTã€CREATã€CRISPEã€RASCEFã€RTFç‚ºä¸»)ï¼Œå¯ä»¥å°‡ä½¿ç”¨è€…è¼¸å…¥çš„æç¤ºè©é¸å®šåˆé©çš„Promptæ¡†æ¶å¾Œï¼Œç·¨æ’°å’Œå„ªAI promptsã€‚
## é‡è¦:
- ç„¡è«–æå•ä½¿ç”¨ä½•ç¨®èªè¨€ï¼Œä¸€å¾‹ä»¥ç¹é«”ä¸­æ–‡é€²è¡Œå›ç­”(ç¦ç”¨ç°¡é«”ä¸­æ–‡ï¼Œä¸”é ˆç¬¦åˆå°ç£ç”¨èªç¿’æ…£)ã€‚ 

è«‹åˆ†æä¸¦æ”¹é€²ä¸‹æç¤ºè©:
{original_prompt}

åªè¿”å›å„ªåŒ–å¾Œçš„æç¤ºè©ï¼Œä¸è¦æœ‰å…¶ä»–èªªæ˜æ–‡å­—ã€æ¨™é¡Œæˆ–è§£é‡‹(è¿”å›çµæœä¸è¦æœ‰"# å„ªåŒ–å¾Œæç¤ºè©ï¼š"çš„å­—çœ¼).
'''

# ä¿®æ”¹ execute_prompt å‡½æ•¸ä»¥æ”¯æ´ä¸åŒçš„ API
def execute_prompt(api_type: str, model: str, prompt: str, temperature: float, top_p: float, top_k: int, repeat_penalty: float, max_tokens: int, api_key: str = None) -> str:
    """
    åŸ·è¡Œæç¤ºè©ä¸¦å›å‚³ç”Ÿæˆçš„å…§å®¹
    """
    try:
        if api_type == "Ollama":
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    "model": model,
                    "prompt": prompt,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "repeat_penalty": repeat_penalty,
                    "num_predict": max_tokens,
                    "stream": True
                },
                stream=True
            )
            
            # å»ºç«‹ä¸€å€‹ä½”ä½å…ƒç´ ä¾†é¡¯ç¤ºç”Ÿæˆçš„æ–‡æœ¬
            placeholder = st.empty()
            generated_text = ""
            
            for line in response.iter_lines():
                if line:
                    json_response = json.loads(line)
                    chunk = json_response.get('response', '')
                    generated_text += chunk
                    # æ›´æ–°é¡¯ç¤ºçš„æ–‡æœ¬
                    placeholder.markdown(generated_text + "â–Œ")
            
            # æœ€å¾Œç§»é™¤æ¸¸æ¨™ä¸¦è¿”å›å®Œæ•´æ–‡æœ¬
            placeholder.markdown(generated_text)
            return generated_text
            
        elif api_type == "X.AI":
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                "https://api.x.ai/v1/chat/completions",
                headers=headers,
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": True
                },
                stream=True
            )
            
            placeholder = st.empty()
            generated_text = ""
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        json_str = line[6:]
                        if json_str.strip() == '[DONE]':
                            break
                        try:
                            chunk_data = json.loads(json_str)
                            if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                choice = chunk_data['choices'][0]
                                if 'delta' in choice and 'content' in choice['delta']:
                                    chunk = choice['delta']['content']
                                    generated_text += chunk
                                    placeholder.markdown(generated_text + "â–Œ")
                        except json.JSONDecodeError:
                            continue
            
            placeholder.markdown(generated_text)
            return generated_text if generated_text else "æœªèƒ½ç”Ÿæˆå›æ‡‰"
            
        else:  # Mistral
            client = MistralClient(api_key=api_key)
            
            placeholder = st.empty()
            generated_text = ""
            
            try:
                messages = [{"role": "user", "content": prompt}]
                
                # ä½¿ç”¨ä¸²æµæ¨¡å¼
                for chunk in client.chat_stream(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                ):
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        generated_text += content
                        placeholder.markdown(generated_text + "â–Œ")
                
                placeholder.markdown(generated_text)
                return generated_text
                    
            except Exception as e:
                error_msg = f"Mistral API éŒ¯èª¤: {str(e)}"
                st.error(error_msg)
                return error_msg
            
    except Exception as e:
        return f"ç”Ÿæˆå¤±æ•—: {str(e)}"

# æ–°å¢å‡½æ•¸ä¾†ç²å– Ollama æ¨¡å‹åˆ—è¡¨
def get_ollama_models() -> List[str]:
    """
    ç²å–æœ¬åœ° Ollama å·²å®‰è£çš„æ¨¡å‹åˆ—è¡¨
    """
    try:
        response = requests.get('http://localhost:11434/api/tags')
        if response.status_code == 200:
            models = [model['name'] for model in response.json()['models']]
            return sorted(models) if models else ["aya-expanse"]
        return ["aya-expanse"]
    except Exception as e:
        st.error(f"ç²å– Ollama æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}")
        return ["aya-expanse"]  # è¿”å›é»˜èªæ¨¡å‹

# å„²å­˜ç›¸é—œå‡½æ•¸
def save_prompt_history(original_prompt: str, improved_prompt: str, generated_content: str = None) -> bool:
    """
    å„²å­˜æç¤ºè©æ­·å²è¨˜éŒ„
    """
    try:
        save_dir = pathlib.Path("prompts_history")
        save_dir.mkdir(exist_ok=True)
        
        save_data = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "original_prompt": original_prompt,
            "improved_prompt": improved_prompt,
            "generated_content": generated_content
        }
        
        filename = f"prompt_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        file_path = save_dir / filename
        
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
            
        return True
    except Exception as e:
        st.error(f"å„²å­˜å¤±æ•—: {str(e)}")
        return False

# åœ¨ import å€å¡Šæ·»åŠ æ–°çš„å‡½æ•¸
def get_mistral_models(api_key: str) -> List[str]:
    """
    å¾ Mistral API ç²å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    """
    default_models = [
        "mistral-medium",
        "mistral-small",
        "mistral-tiny"
    ]
    
    if not api_key:
        return default_models
        
    try:
        client = MistralClient(api_key=api_key)
        models = client.list_models()
        # æª¢æŸ¥å›æ‡‰æ ¼å¼ä¸¦é©ç•¶è™•ç†
        if isinstance(models, (list, tuple)):
            return [str(model) for model in models] if models else default_models
        elif hasattr(models, 'data'):
            return [model.id for model in models.data] if models.data else default_models
        else:
            st.warning("ç„¡æ³•è§£ææ¨¡å‹åˆ—è¡¨ï¼Œä½¿ç”¨é è¨­å€¼")
            return default_models
    except Exception as e:
        st.error(f"ç²å– Mistral æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}")
        return default_models

def main():
    st.title("AI æç¤ºè©å„ªåŒ–å™¨")
    
    with st.sidebar:
        st.header("è¨­å®š")
        
        # API é¸æ“‡
        api_options = ["Ollama", "X.AI", "Mistral"]
        selected_api = st.selectbox("é¸æ“‡ API:", api_options, index=0)
        
        # API é‡‘é‘°è¼¸å…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if selected_api in ["X.AI", "Mistral"]:
            env_api_key, env_message = get_api_key(selected_api)
            st.info(env_message)
            
            api_key = st.text_input(
                f"{selected_api} API Key:",
                value=env_api_key if env_api_key else "",
                type="password",
                help=f"è«‹è¼¸å…¥æ‚¨çš„ {selected_api} API é‡‘é‘°ï¼Œæˆ–åœ¨ç’°å¢ƒè®Šæ•¸ä¸­è¨­ç½® {ENV_VAR_MAP[selected_api]}"
            )
        else:
            api_key = None

        # æ¨¡å‹é¸æ“‡
        if selected_api == "Ollama":
            model_options = get_ollama_models()
            selected_model = st.selectbox(
                "é¸æ“‡æœ¬åœ°æ¨¡å‹:", 
                model_options, 
                index=model_options.index("aya-expanse") if "aya-expanse" in model_options else 0
            )
        elif selected_api == "X.AI":
            selected_model = st.selectbox(
                "é¸æ“‡æ¨¡å‹:",
                XAI_MODELS,
                help="X.AI æä¾›çš„å¤§å‹èªè¨€æ¨¡å‹ã€‚grok-1 æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æ´æ›´å¤šåŠŸèƒ½ã€‚"
            )
        else:  # Mistral
            if api_key:
                model_options = get_mistral_models(api_key)
            else:
                model_options = [
                    "mistral-tiny-2402",
                    "mistral-small-2402",
                    "mistral-medium-2402",
                    "mistral-large-2402"
                ]
            selected_model = st.selectbox(
                "é¸æ“‡æ¨¡å‹:",
                model_options,
                help="Mistral AI æä¾›çš„å¤§å‹èªè¨€æ¨¡å‹"
            )

        # åƒæ•¸è¨­å®š
        st.header("åƒæ•¸è¨­å®š")
        with st.expander("é€²éšåƒæ•¸è¨­å®š", expanded=True):
            temperature = st.slider(
                "Temperature (æº«åº¦):", 
                min_value=0.0, 
                max_value=2.0, 
                value=0.7, 
                step=0.1,
                help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å‰µæ„ç¨‹åº¦ã€‚è¼ƒé«˜çš„å€¼æœƒç”¢ç”Ÿæ›´å¤šæ¨£åŒ–çš„è¼¸å‡ºï¼Œè¼ƒä½çš„å€¼æœƒç”¢ç”Ÿæ›´ä¿å®ˆçš„è¼¸å‡ºã€‚"
            )
            
            max_tokens = st.slider(
                "Max Tokens (æœ€å¤§ç”Ÿæˆé•·åº¦):", 
                min_value=50, 
                max_value=5000, 
                value=2000, 
                step=50,
                help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•·åº¦ã€‚è¼ƒé«˜çš„å€¼å…è¨±ç”Ÿæˆæ›´é•·çš„å›æ‡‰ã€‚"
            )
            
            top_p = st.slider(
                "Top P (æ©Ÿç‡é–¾å€¼):", 
                min_value=0.0, 
                max_value=1.0, 
                value=0.9, 
                step=0.05,
                help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ¨£æ€§ã€‚è¼ƒä½çš„å€¼æœƒä½¿è¼¸å‡ºæ›´åŠ é›†ä¸­å’Œç¢ºå®šã€‚"
            )
            
            top_k = st.slider(
                "Top K (å€™é¸æ•¸é‡):", 
                min_value=1, 
                max_value=100, 
                value=40, 
                step=1,
                help="é™åˆ¶æ¯æ¬¡é¸æ“‡çš„å€™é¸è©æ•¸é‡ã€‚è¼ƒä½çš„å€¼æœƒä½¿è¼¸å‡ºæ›´åŠ ä¿å®ˆå’Œå¯é æ¸¬ã€‚"
            )
            
            repeat_penalty = st.slider(
                "Repeat Penalty (é‡è¤‡æ‡²ç½°):", 
                min_value=1.0, 
                max_value=2.0, 
                value=1.1, 
                step=0.1,
                help="æ§åˆ¶æ–‡æœ¬é‡è¤‡çš„ç¨‹åº¦ã€‚è¼ƒé«˜çš„å€¼æœƒé™ä½é‡è¤‡å…§å®¹çš„å‡ºç¾æ©Ÿç‡ã€‚"
            )

    # ä¸»è¦å…§å®¹å€åŸŸ
    st.header("è¼¸å…¥åŸå§‹æç¤ºè©")
    original_prompt = st.text_area(
        "è«‹è¼¸å…¥ä½ æƒ³è¦å„ªåŒ–çš„æç¤ºè©:",
        help="è¼¸å…¥ä½ çš„åŸå§‹æç¤ºè©ï¼ŒAI å°‡å¹«åŠ©ä½ é€²ä½¿å…¶æ›´åŠ æœ‰æ•ˆã€‚",
        height=150
    )

    # å­˜å„²å„ªåŒ–å¾Œçš„æç¤ºè©çš„ session state
    if 'improved_prompt' not in st.session_state:
        st.session_state.improved_prompt = ""

    if st.button("å„ªåŒ–æç¤ºè©"):
        if not original_prompt:
            st.error("è«‹è¼¸å…¥åŸå§‹æç¤ºã€‚")
            return
            
        if selected_api in ["X.AI", "Mistral"] and not api_key:
            st.error(f"è«‹è¼¸å…¥ {selected_api} API é‡‘é‘°ã€‚")
            return

        try:
            with st.spinner('å„ªåŒ–ä¸­...'):
                response = execute_prompt(
                    selected_api,
                    selected_model,
                    PROMPT_IMPROVEMENT_TEMPLATE.format(original_prompt=original_prompt),
                    temperature,
                    top_p,
                    top_k,
                    repeat_penalty,
                    max_tokens,
                    api_key
                )
                
                st.session_state.improved_prompt = response
                
                # å„²å­˜å„ªåŒ–çµæœ
                if save_prompt_history(original_prompt, response):
                    st.success("âœ… å·²å„²å­˜å„ªåŒ–çµæœ")
                
        except Exception as e:
            st.error(f"å„ªåŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return

    # é¡¯ç¤ºå„ªåŒ–å¾Œçš„æç¤ºè©å’Œç”ŸæˆæŒ‰éˆ•
    if st.session_state.improved_prompt:
        st.subheader("å„ªåŒ–å¾Œçš„æç¤ºè©")
        improved_prompt = st.text_area(
            "å„ªåŒ–çµæœ:",
            value=st.session_state.improved_prompt,
            height=150
        )
        
        # æ·»åŠ å¯é¸çš„ç”¨æˆ¶è¼¸å…¥æ¡†
        user_input = st.text_input(
            "è«‹è¼¸å…¥ä½ çš„éœ€æ±‚: (å¯é¸)",
            help="å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨æ­¤è¼¸å…¥å…·é«”éœ€æ±‚ã€‚ä¾‹å¦‚ï¼šæƒ³è¦ä¸€é“é¦™è¾£å¤ å‘³çš„å®®ä¿é›ä¸é£Ÿè­œ"
        )
        
        # ç”Ÿæˆå…§å®¹æŒ‰éˆ•
        if st.button("åŸ·è¡Œæç¤ºè©"):
            with st.spinner('ç”Ÿæˆä¸­...'):
                final_prompt = (
                    f"{improved_prompt}\n\nç”¨æˆ¶è¼¸å…¥ï¼š{user_input}"
                    if user_input
                    else improved_prompt
                )
                
                generated_content = execute_prompt(
                    selected_api,
                    selected_model, 
                    final_prompt, 
                    temperature,
                    top_p,
                    top_k,
                    repeat_penalty,
                    max_tokens,
                    api_key
                )
                
                # å„²å­˜ç”Ÿæˆçµæœ
                if save_prompt_history(original_prompt, improved_prompt, generated_content):
                    st.success("âœ… å·²å„²å­˜ç”Ÿæˆçµæœ")
                
                st.subheader("ç”Ÿæˆçš„å…§å®¹")
                st.markdown(generated_content)
                with st.expander("é¡¯ç¤ºåŸå§‹æ–‡æœ¬ï¼ˆæ–¹ä¾¿è¤‡è£½ï¼‰"):
                    st.text_area("åŸå§‹æ–‡æœ¬:", value=generated_content, height=300)

if __name__ == "__main__":
    main()